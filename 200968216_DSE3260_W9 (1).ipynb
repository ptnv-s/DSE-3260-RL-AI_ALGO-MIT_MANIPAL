{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSE 3260\n",
        "## Week - 9\n",
        "### Reg. No - 200968216\n",
        "### Pratinav Seth"
      ],
      "metadata": {
        "id": "LZho6cwMYdbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-BiWFS5W3Sg"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CliffWalking-v0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R30-SLnVXFoX",
        "outputId": "96f84b76-8fa0-486c-a19f-f6bd0c619d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 100\n",
        "gamma = 1.0\n",
        "epsilon = 0.1\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n"
      ],
      "metadata": {
        "id": "QqtYaglmXKtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_mc_es = np.zeros((num_states, num_actions))\n",
        "returns_sum = np.zeros((num_states, num_actions))\n",
        "returns_count = np.zeros((num_states, num_actions))\n",
        "\n",
        "Q_on_policy = np.zeros((num_states, num_actions))\n",
        "returns_sum_on_policy = np.zeros((num_states, num_actions))\n",
        "returns_count_on_policy = np.zeros((num_states, num_actions))\n"
      ],
      "metadata": {
        "id": "o7YtrQpwXQub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i_episode in range(num_episodes):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.random.choice(num_actions)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    \n",
        "    states, actions, rewards = zip(*episode)\n",
        "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
        "    \n",
        "    for i, state in enumerate(states):\n",
        "        action = actions[i]\n",
        "        returns_sum[state][action] += sum(rewards[i:] * discounts[:-(1+i)])\n",
        "        returns_count[state][action] += 1.0\n",
        "        Q_mc_es[state][action] = returns_sum[state][action] / returns_count[state][action]"
      ],
      "metadata": {
        "id": "8QEPgiExXXQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_soft_policy(state, Q, epsilon):\n",
        "    policy = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
        "    best_action = np.argmax(Q[state])\n",
        "    policy[best_action] += (1.0 - epsilon)\n",
        "    return policy"
      ],
      "metadata": {
        "id": "Erx9LKsjXn6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i_episode in range(num_episodes):\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        policy = epsilon_soft_policy(state, Q_on_policy, epsilon)\n",
        "        action = np.random.choice(num_actions, p=policy)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "        state = next_state\n",
        "    \n",
        "    discounts = gamma ** np.arange(len(episode_rewards) + 1)\n",
        "    returns = np.cumsum(episode_rewards[::-1])[::-1] * discounts[:-1]\n",
        "    for t in range(len(episode_states)):\n",
        "        state = episode_states[t]\n",
        "        action = episode_actions[t]\n",
        "        returns_sum_on_policy[state][action] += returns[t]\n",
        "        returns_count_on_policy[state][action] += 1\n",
        "        Q_on_policy[state][action] = returns_sum_on_policy[state][action] / returns_count_on_policy[state][action]"
      ],
      "metadata": {
        "id": "yRh0SFmTXsDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal Policy using Monte Carlo ES:\\n\", np.argmax(Q_mc_es, axis=1))\n",
        "print(\"\\nOptimal Policy using on-policy first-visit MC control:\\n\", np.argmax(Q_on_policy, axis=1))"
      ],
      "metadata": {
        "id": "xrC87YwSXvoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare  and comment  on  the  performance  of  both  methods  in  terms  of  number  of  steps needed to learn optimal policy and the number of episodes\n",
        "\n",
        "In general, the Monte Carlo ES approach requires a large number of episodes to converge to the best policy. This is due to the fact that it must visit each state-action pair frequently enough to accurately estimate the Q-values. As a result, learning the optimal policy may necessitate a significant number of events.\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "First-time policy visit The MC control for epsilon-soft policies generates episodes based on an epsilon-soft policy, which means it chooses the best action with probability = (1-epsilon) and a random action with probability epsilon. This allows the algorithm to explore the state-action space while also utilising the best policy at the time. When compared to the Monte Carlo ES method, the on-policy first-visit MC control method requires fewer episodes to converge. This is due to the fact that it improves the policy gradually while creating episodes and updates the Q-values after each episode.\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "In terms of the number of steps required to learn the optimal policy, assuming convergence, both techniques should learn the best policy in a similar number of steps. The amount of steps necessary for convergence, however, can vary depending on the problem and the hyperparameters used.\n"
      ],
      "metadata": {
        "id": "CtP6UdgxdXYn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adV9kKpHxYHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}