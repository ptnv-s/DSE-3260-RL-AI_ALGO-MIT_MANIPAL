{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSE 3260\n",
        "## Week - 5\n",
        "### Reg. No - 200968216\n",
        "#### Pratinav Seth "
      ],
      "metadata": {
        "id": "GU5TyN8dd63f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do the tutorial for MAB in TF-Agents\n",
        "\n",
        "Link - https://www.tensorflow.org/agents/tutorials/bandits_tutorial"
      ],
      "metadata": {
        "id": "0_xDmEKgoxOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKmA36NP2bzP",
        "outputId": "d387b3c1-d2d6-4111-9bab-51c87293c0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "metadata": {
        "id": "Hkris7LeJbjJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 - Create a  environment \n",
        "  a. for  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.<br>\n",
        "  b. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.<<br>\n",
        "  c. Request  for  50  observations  from  the  environment,  compute  and  print  the total reward."
      ],
      "metadata": {
        "id": "QGAvNdwQeoOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  def _reset(self):\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\"\"\"\n"
      ],
      "metadata": {
        "id": "_0UVBgxVJgsY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5, 6, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ],
      "metadata": {
        "id": "ynGHPCLlJr7Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = SimplePyEnvironment()\n",
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "metadata": {
        "id": "TAB9BNtAJy8r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "Cem8n3EtJ5jY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "for i in range(50):\n",
        "  current_time_step = tf_environment.reset()\n",
        "  action = sign_policy.action(current_time_step).action\n",
        "  reward = tf_environment.step(action).reward\n",
        "  total_reward += reward\n",
        "\n",
        "print(f'Total Reward: {total_reward}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRuFrO24J8PQ",
        "outputId": "7076242a-8629-4e20-82b3-cfd456bb4988"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: [[134.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step = tf_environment.reset()\n",
        "action = 1\n",
        "next_step = tf_environment.step(action)\n",
        "reward = next_step.reward\n",
        "next_observation = next_step.observation\n",
        "print(\"Reward: \")\n",
        "print(reward)\n",
        "print(\"Next observation:\")\n",
        "print(next_observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLjwSavoKD4b",
        "outputId": "86c3a575-0208-4f53-8b61-f5cd543fc33c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward: \n",
            "tf.Tensor([[3.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[5]], shape=(1, 1), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 â€“Create an environment \n",
        "a. Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.<br>\n",
        "b. Define a policy that detects the behaviorof the underlying environment. There are three situations that the policy needs to handle:\n",
        " <li>i.The agent has not detected know yet which version of the environment is running.\n",
        " <li>ii.The  agent  detected  that  the  original  version  of  the  environment  is running.\n",
        " <li>iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running.<br>\n",
        "c. Define the agent that detects the sign of the environment and sets the policy appropriately."
      ],
      "metadata": {
        "id": "O0P8g622fRDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-2, maximum=2, name='observation')\n",
        "\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmkSuQMDKNHc",
        "outputId": "224d022c-69fc-45bd-abee-63f010705e8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward sign:\n",
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "bws-zkoSKS8b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())"
      ],
      "metadata": {
        "id": "0z1R4YG_KWtO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = TwoWayPyEnvironment()\n",
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JbQ_KvtsUdt",
        "outputId": "da1284fb-fdfc-4419-d514-46894088622e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward sign:\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sign_agent = SignAgent()"
      ],
      "metadata": {
        "id": "w5Q4ooVtsU5B"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}