{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSE 3260\n",
        "## Week - 6\n",
        "### Reg. No - 200968216\n",
        "#### Pratinav Seth "
      ],
      "metadata": {
        "id": "-WWCxFFHAGCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1. MAB agent formulation :\n",
        "##### The problem agent formulation involves determining the most optimal ad to display to a user at a given time instant to maximize the number of clicks on the webpage. \n",
        "\n",
        "The problem can be defined as :\n",
        "\n",
        " -- There are 10 different ads to choose from, and at each time step, the MAB agent must decide which ad to display to the user. \n",
        "\n",
        " -- Goal is to maximize the total number of clicks obtained from the users over a specified time horizon. \n",
        "\n",
        " -- Each ad has an unknown click-through rate (CTR) that represents the probability of a user clicking on that ad. \n",
        " \n",
        " -- The MAB agent's objective is to learn the true CTR of each ad while minimizing the regret, which is the difference between the expected number of clicks obtained by displaying the best ad and the expected number of clicks obtained by displaying the chosen ad at each time step. \n",
        " \n",
        " -- The MAB agent must balance the exploration of less-known ads to learn their CTRs with the exploitation of the ads that are known to have higher CTRs to maximize the total number of clicks."
      ],
      "metadata": {
        "id": "j1NP_K3rAGY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "ads_clicks = pd.read_csv('/content/Ads_Optimisation.csv')"
      ],
      "metadata": {
        "id": "0rkwk4WIDyEL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_ads = len(ads_clicks.columns)\n",
        "print(\"Number of ads - \" + str(num_ads))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4fFqlsmE80o",
        "outputId": "101767c8-f875-4fd1-8129-17f8f203a4c0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of ads - 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2. Total rewards after 2000-time steps using the ε-greedy algorithm for ε=\n",
        "- 0.01 \n",
        "\n",
        "- 0.3"
      ],
      "metadata": {
        "id": "BE2iRcpraSe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(epsilon, rewards):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        ad = random.randint(0, num_ads - 1)\n",
        "    else:\n",
        "        ad = np.argmax(rewards)\n",
        "    return ad"
      ],
      "metadata": {
        "id": "v3fVLdk5Ltvc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = [0] * num_ads\n",
        "total_rewards_01 = []\n",
        "total_rewards_03 = []"
      ],
      "metadata": {
        "id": "D8uusnFeLx6m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterating the ε-greedy algorithm for 2000 time steps using ε=0.01 and ε=0.3"
      ],
      "metadata": {
        "id": "S13TeXY2q5LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(2000):\n",
        "    ad_01 = epsilon_greedy(0.01, rewards)\n",
        "    ad_03 = epsilon_greedy(0.3, rewards)\n",
        "    reward = ads_clicks.iloc[t][ad_01]\n",
        "    rewards[ad_01] = rewards[ad_01] + reward\n",
        "    total_rewards_01.append(sum(rewards))\n",
        "    reward = ads_clicks.iloc[t][ad_03]\n",
        "    rewards[ad_03] = rewards[ad_03] + reward\n",
        "    total_rewards_03.append(sum(rewards))"
      ],
      "metadata": {
        "id": "M6Y1Wo5xMMAe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total rewards for ε - 0.01: \", total_rewards_01[-1])\n",
        "print(\"Total rewards for ε - 0.3: \", total_rewards_03[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb06mc1mMPy_",
        "outputId": "03964608-280a-4a8f-f7ac-ab89b828191e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards for ε - 0.01:  650\n",
            "Total rewards for ε - 0.3:  650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c = 1.5"
      ],
      "metadata": {
        "id": "mARxbQyIZ901"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ucb(rewards, n, t, c=1.5):\n",
        "    average_rewards = rewards / n\n",
        "    ucb_values = average_rewards + c * np.sqrt(np.log(t + 1) / n)\n",
        "    ad = np.argmax(ucb_values)\n",
        "    return ad"
      ],
      "metadata": {
        "id": "Peu_lxpORHAk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = np.zeros(num_ads)\n",
        "n = np.zeros(num_ads)\n",
        "total_rewards = []"
      ],
      "metadata": {
        "id": "4HYRi5R9U59r"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(2000):\n",
        "    ad = ucb(rewards, n, t, c=1.5)    \n",
        "    reward = ads_clicks.iloc[t][ad]\n",
        "    rewards[ad] = rewards[ad] + reward\n",
        "    n[ad] = n[ad] + 1\n",
        "    total_rewards.append(sum(rewards))\n",
        "\n",
        "print(\"Total rewards for c=1.5: \", total_rewards[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8ZugbLbU-tE",
        "outputId": "6d1d4d03-a48e-43a7-c248-959e1a73a413"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards for c=1.5:  763.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the epilson-greedy strategy, how does the estimated action value compare to the optimal action:\n",
        "\n",
        "The epilson-greedy technique estimates action value using the sample average method, with the estimated value of each action equaling the average of the rewards received for that action. However, if the number of samples is minimal, the sample average approach can produce considerable variance in the estimate. Thereby, the estimations may not converge to the genuine action values soon. As a result, the epilson -greedy strategy prefers to explore more in the start of the experiment to reduce uncertainty, and then exploit the optimum action later on based on the projected action values.\n",
        "\n",
        "\n",
        "\n",
        "#### How the action value estimated compares to the ideal action in the UCB approach:\n",
        "\n",
        "The Upper-Confidence-Bound strategy, on the other hand, uses the Upper Confidence Bound (UCB), which is a combination of the average reward and an uncertainty term, to estimate the action values.\n",
        "\n",
        "The uncertainty term ensures that activities that have not been chosen many times are given a higher priority, whereas actions that have been chosen many times are given a lower priority. This method yields a more stable estimate of the action values, and the algorithm converges to the optimal action faster.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sSTFSOD7XvjU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOtqrnQzssMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}