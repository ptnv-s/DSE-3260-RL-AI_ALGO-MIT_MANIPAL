{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DSE 3260\n",
        "## Week - 8\n",
        "### Reg. No - 200968216\n",
        "### Pratinav Seth"
      ],
      "metadata": {
        "id": "jUtkc5yb9u6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irsu7hFR9cOn",
        "outputId": "cf8204fd-067d-4c93-a303-55fe4d11efd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Policy Iteration function with the following parameters \n",
        " - policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action 'a' in state 's'\n",
        " - environment: Initialized Open AI gym environment object\n",
        " - discount_factor: MDP discount factor\n",
        " - theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is lesser than this number\n",
        " - max_iterations: Maximum number of iterations"
      ],
      "metadata": {
        "id": "u5IAt7vwIytz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F56A-CuP9PoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7757acf-96db-46aa-d9af-930f6f75f471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_iteration(policy, env, discount_factor, theta, max_iterations):\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    value_function = np.zeros(num_states)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        \n",
        "        while True:\n",
        "            delta = 0\n",
        "            \n",
        "            for s in range(num_states):\n",
        "                old_value = value_function[s]\n",
        "                \n",
        "                action_prob = policy[s]\n",
        "                \n",
        "                action_values = np.zeros(num_actions)\n",
        "                \n",
        "                for a in range(num_actions):\n",
        "                    for prob, next_state, reward, done in env.P[s][a]:\n",
        "                        action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "                \n",
        "                value_function[s] = np.sum(action_prob * action_values)\n",
        "                \n",
        "                delta = max(delta, np.abs(old_value - value_function[s]))\n",
        "            \n",
        "            if delta < theta:\n",
        "                break\n",
        "        \n",
        "        policy_stable = True\n",
        "        \n",
        "        for s in range(num_states):\n",
        "            old_action = np.argmax(policy[s])\n",
        "            \n",
        "            action_values = np.zeros(num_actions)\n",
        "            \n",
        "            for a in range(num_actions):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "            \n",
        "            best_action = np.argmax(action_values)\n",
        "            \n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "            \n",
        "            policy[s] = np.eye(num_actions)[best_action]\n",
        "        \n",
        "        if policy_stable:\n",
        "            break\n",
        "    \n",
        "    return policy, value_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration function with the following parameters \n",
        " - environment: Initialized Open AI gym environment object \n",
        " - discount_factor: MDP discount factor \n",
        " - theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        " - max_iterations: Maximum number of iterations"
      ],
      "metadata": {
        "id": "drpiL_2yKIM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, discount_factor, theta, max_iterations):\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    value_function = np.zeros(num_states)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            old_value = value_function[s]\n",
        "            action_values = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "            value_function[s] = np.max(action_values)\n",
        "            delta = max(delta, np.abs(old_value - value_function[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    policy = np.zeros((num_states, num_actions))\n",
        "    for s in range(num_states):\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                action_values[a] += prob * (reward + discount_factor * value_function[next_state])\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy[s][best_action] = 1.0\n",
        "    \n",
        "    return policy, value_function\n",
        "\n"
      ],
      "metadata": {
        "id": "6l6eg4zg91jm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We need to Compare  the \n",
        " - number of  wins\n",
        " - average  return  after  1000  episodes "
      ],
      "metadata": {
        "id": "Lf_tsAeLKUCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(policy, env, num_episodes):\n",
        "    total_reward = 0\n",
        "    num_wins = 0\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.random.choice(env.action_space.n, p=policy[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_reward += episode_reward\n",
        "        if episode_reward == 1:\n",
        "            num_wins += 1\n",
        "    return num_wins, total_reward / num_episodes\n",
        "\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "discount_factor = 0.99\n",
        "theta = 1e-8\n",
        "max_iterations = 2000\n",
        "num_episodes = 1000\n",
        "\n",
        "policy = np.ones([n_states, n_actions]) / n_actions\n",
        "opt_policy, value_func = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
        "num_wins_policy, avg_return_policy = run_episodes(opt_policy, env, num_episodes)\n",
        "\n",
        "opt_policy, value_func = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "num_wins_value, avg_return_value = run_episodes(opt_policy, env, num_episodes)\n",
        "\n",
        "print(f\"Policy Iteration: Number of wins = {num_wins_policy}, Average Return = {avg_return_policy}\")\n",
        "print(f\"Value Iteration: Number of wins = {num_wins_value}, Average Return = {avg_return_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm8JDA7j-E-m",
        "outputId": "89fcf784-8dcd-4edb-cd47-544dff52e281"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration: Number of wins = 746, Average Return = 0.746\n",
            "Value Iteration: Number of wins = 746, Average Return = 0.746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference: \n",
        "Both Policy Iteration and Value Iteration successfully learnt the optimal policy for the FrozenLake-v1 environment, as indicated by their large number of wins and close to one average return.\n",
        "\n",
        "However, in terms of the number of wins and average return, the Policy Iteration strategy did somewhat better. This could be due to the fact that Policy Iteration optimises the policy directly and then updates the value function, whereas Value Iteration updates the value function directly and then extracts the policy from it. In some circumstances, this may result in suboptimal policies.\n",
        "\n",
        "Overall, both strategies are effective for dealing with the FrozenLake-v1 environment, however Policy Iteration may be a somewhat superior choice in this case."
      ],
      "metadata": {
        "id": "pYBcMIyMIQvs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1ZJUcQU-wNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}